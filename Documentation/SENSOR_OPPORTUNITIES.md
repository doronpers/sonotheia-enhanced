# Underutilized Opportunities & New Sensor Detection Methods

**Analysis Date:** December 9, 2025 (Revised)
**Status:** Trial Implementation Recommendations
**Priority:** CRITICAL - Accuracy Enhancement Initiative

---

## üö® CRITICAL FINDING: Detection Pipeline Failures

### Deepfake Detection Rate: 0%

**The calibration data reveals a critical issue**: All 3 deepfake test samples were incorrectly APPROVED despite the sensors correctly identifying them as suspicious.

| Sample | Voice Risk | Composite Risk | Expected | Actual |
|--------|------------|----------------|----------|--------|
| CAL-SUSPICIOUS-000 | 0.994 | 0.497 | ESCALATE | APPROVE ‚ùå |
| CAL-SUSPICIOUS-001 | 0.759 | 0.380 | ESCALATE | APPROVE ‚ùå |
| CAL-SUSPICIOUS-002 | 0.960 | 0.480 | ESCALATE | APPROVE ‚ùå |

**Root Cause**: The `composite_risk_score` is calculated as `voice_risk * 0.5`, which dilutes high-risk scores below the ESCALATE threshold of 0.5.

```python
# From baseline_test.py - Current decision logic:
if composite_risk < 0.5:
    decision = "APPROVE"  # <-- Deepfakes with 0.38-0.50 composite fall here!
elif composite_risk < 0.7:
    decision = "ESCALATE"
```

### Recommended Fix (IMMEDIATE)

```python
# Option 1: Lower the ESCALATE threshold
if composite_risk < 0.3:
    decision = "APPROVE"
elif composite_risk < 0.35:  # Changed from 0.5
    decision = "APPROVE"
elif composite_risk >= 0.35:  # Catches 0.38-0.50 scores
    decision = "ESCALATE"

# Option 2: Remove the 0.5 dilution factor
composite_risk = voice_risk  # Not voice_risk * 0.5
```

---

## Executive Summary

This document identifies **underutilized detection opportunities** and **new sensor methods** that could significantly improve Sonotheia's deepfake detection accuracy. The revised analysis reveals:

1. **üö® CRITICAL**: Decision logic bug causing 0% deepfake detection rate
2. **3 existing sensors not activated** in the default pipeline (ENF, Breathing Pattern, PhaseCoherence)
3. **4 sensors disabled** with zero weight (Two-Mouth, HFDeepfake, HFEnsemble, RawNet3)
4. **Insufficient calibration**: Only 14 test samples (3 deepfakes)
5. **4 new sensor candidates** based on speech science literature

**Estimated Impact:** Fixing the decision threshold AND activating dormant sensors could improve detection accuracy from 0% to 70-85%.

---

## Part 1: Underutilized Existing Sensors

### 1.1 ENF Sensor (Electrical Network Frequency) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** Implemented but NOT in default pipeline
**Location:** `backend/sensors/enf.py` (327 lines)
**Trial Priority:** HIGH

#### The Opportunity

The ENF sensor is fully implemented but **not registered** in `get_default_sensors()`. This is a significant missed opportunity because:

- **Unique differentiator**: No known competitor uses ENF for voice deepfake detection
- **Complementary signal**: Detects environmental authenticity, not voice characteristics
- **Hard to spoof**: Attackers would need to synthesize correct grid frequency variations

#### How It Works

```
Real recordings from electrical devices contain 50/60Hz power grid frequency
embedded in the audio. This frequency varies slightly over time with grid load.

Detection Strategy:
1. Extract narrow-band signal around 50/60Hz
2. Analyze frequency stability (real = natural variations)
3. Check phase continuity (synthetic = discontinuities)
4. Compare to known ENF databases (future: geolocation verification)
```

#### Patent Safety

‚úÖ **SAFE** - ENF analysis is:
- Not covered by Pindrop patents (focuses on voice, not environmental signals)
- Uses frequency-domain analysis, not LPC
- Independent of source-filter modeling

#### Recommended Trial Implementation

```python
# In backend/sensors/registry.py, add to get_default_sensors():
from .enf import ENFSensor

return [
    # ... existing sensors ...
    ENFSensor(category="defense"),  # Trust (Environmental Authenticity)
]
```

#### Expected Benefits

| Metric | Current | With ENF |
|--------|---------|----------|
| Mobile recordings detection | Medium | High |
| Studio/clean deepfakes | Weak | Strong |
| Geographic verification | None | Possible |

#### Caveats

- Requires minimum 2 seconds of audio
- May not work for recordings made on battery-powered devices
- Regional configuration needed (50Hz vs 60Hz)

---

### 1.2 Breathing Pattern Sensor (Regularity Analysis) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** Implemented but NOT in default pipeline
**Location:** `backend/sensors/breathing_pattern.py` (335 lines)
**Trial Priority:** HIGH

#### The Opportunity

This sensor analyzes breathing **regularity**, complementing the existing `BreathSensor` which checks phonation duration. Key insight:

- **Real breathing**: Naturally irregular timing (coefficient of variation > 0.3)
- **Synthetic breathing**: Often unnaturally regular or absent

#### How It Works

```
1. Bandpass filter audio to 20-300Hz (breathing frequency range)
2. Detect breath events using energy envelope peaks
3. Calculate inter-breath interval variance
4. High variance = irregular = authentic
5. Low variance = regular = synthetic

Includes SNR gating to reject noisy recordings.
```

#### Patent Safety

‚úÖ **SAFE** - Uses:
- Spectral filtering (not LPC)
- Temporal pattern analysis (not source-filter)
- Energy envelope detection

#### Recommended Trial Implementation

```python
# In backend/sensors/registry.py:
from .breathing_pattern import BreathingPatternSensor

return [
    # ... existing sensors ...
    BreathingPatternSensor(category="defense"),  # Trust (Natural Variability)
]
```

#### Expected Benefits

| Scenario | Current Detection | With Breathing Pattern |
|----------|-------------------|------------------------|
| TTS with no breathing | Medium (breath sensor) | High |
| TTS with regular breathing | Low | High |
| Clone with irregular breathing | Correct (REAL) | Correct (REAL) |

---

### 1.3 Two-Mouth Sensor (Needs Calibration) ‚≠ê‚≠ê‚≠ê

**Status:** In pipeline but effectively disabled (50% accuracy)
**Location:** `backend/sensors/two_mouth.py` (189 lines)
**Trial Priority:** MEDIUM (after calibration)

#### The Problem

The Two-Mouth sensor detects "anatomical state conflicts" where acoustic features imply contradictory physiological states. However, current thresholds cause:

- 50% false positive rate on real speech
- Disabled in production via arbiter rules

#### Root Cause Analysis

```python
# Current thresholds (too aggressive):
score = min(1.0, max(0.0, (cv - 0.10) * 10))  # VTL variance
score = min(1.0, max(0.0, (conflict_rate - 0.05) * 10))  # Spectral conflict
```

The 0.10 CV threshold for VTL variance is too tight‚Äîreal speech with emotion, emphasis, or prosodic variation can exceed this.

#### Recommended Calibration

1. **Relax VTL variance threshold**: `0.10 ‚Üí 0.15`
2. **Relax spectral conflict rate**: `0.05 ‚Üí 0.08`
3. **Reduce weight in fusion**: `0.15 ‚Üí 0.08`
4. **Add voice activity gating**: Only analyze voiced segments

```python
# Proposed threshold adjustments:
score = min(1.0, max(0.0, (cv - 0.15) * 8))  # More permissive
score = min(1.0, max(0.0, (conflict_rate - 0.08) * 8))
```

#### Expected Impact After Calibration

| Metric | Before | After |
|--------|--------|-------|
| False Positive Rate | 50% | <15% |
| True Positive Rate | High | High (maintained) |
| Usable in Production | No | Yes |

---

### 1.4 Low-Weight Sensors Analysis

#### Coarticulation Sensor (Weight: 0.05)

**Current State:** Active but minimal influence
**Opportunity:** May be undervalued

The coarticulation sensor detects impossible articulation speeds‚Äîa strong synthetic indicator. Current low weight may be due to:
- Conservative tuning during initial development
- Lack of calibration data

**Recommendation:** Trial at 0.10 weight with A/B testing.

#### Breath Sensor (Weight: 0 ‚Üí fallback 0.30)

**Current State:** Using fallback weight system
**Opportunity:** Simplify weight configuration

**Recommendation:** Set explicit weight of 0.15 instead of relying on fallback logic.

#### Bandwidth Sensor (Weight: 0 ‚Üí fallback 0.20)

**Current State:** Using fallback weight system
**Opportunity:** Clarify sensor's role

**Recommendation:** Set explicit weight of 0.10 for full-bandwidth audio quality check.

---

## Part 2: New Sensor Candidates for Trial Implementation

### 2.1 Spectral Harmonicity Sensor (HNR) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** Not implemented (mentioned in ROADMAP.md)
**Effort:** Medium (2-3 days)
**Trial Priority:** HIGHEST

#### The Physics

Human voice has a characteristic **Harmonic-to-Noise Ratio (HNR)**:
- Real voice: HNR typically 8-25 dB for vowels
- TTS artifacts: Often show abnormally high HNR (too clean)
- Voice conversion: May show abnormally low HNR (artifacts as noise)

#### Detection Strategy

```python
class SpectralHarmonicitySensor(BaseSensor):
    """
    Measures Harmonic-to-Noise Ratio to detect TTS artifacts.

    The Physics:
    - Human voice has natural jitter/shimmer creating baseline noise
    - TTS often produces "too perfect" harmonics (high HNR)
    - Voice conversion may introduce artifacts (low HNR)

    Detection:
    - Extract F0 contour using autocorrelation
    - Separate harmonic and noise components
    - Calculate HNR per frame
    - Flag if HNR variance is too low (synthetic consistency)
    """

    def analyze(self, audio_data: np.ndarray, samplerate: int) -> SensorResult:
        # 1. Extract F0 using autocorrelation (NOT LPC)
        f0_contour = self._extract_f0_autocorrelation(audio_data, samplerate)

        # 2. Calculate frame-wise HNR
        hnr_values = self._calculate_hnr(audio_data, samplerate, f0_contour)

        # 3. Analyze HNR statistics
        mean_hnr = np.mean(hnr_values)
        hnr_variance = np.var(hnr_values)

        # 4. Flag synthetic patterns:
        #    - Abnormally high mean HNR (too clean)
        #    - Abnormally low HNR variance (too consistent)
        synthetic_score = 0.0
        if mean_hnr > 20.0:  # Too clean
            synthetic_score += 0.4
        if hnr_variance < 5.0:  # Too consistent
            synthetic_score += 0.4

        return SensorResult(...)
```

#### Patent Safety

‚úÖ **SAFE**:
- Uses autocorrelation for F0 (not LPC)
- Analyzes noise characteristics (not source-filter)
- Measures statistical properties (not spectral templates)

#### Expected Impact

| Deepfake Type | Current Detection | With HNR |
|---------------|-------------------|----------|
| High-quality TTS | Medium | High |
| Neural vocoders | Low | High |
| Voice conversion | Medium | High |

---

### 2.2 Micro-Prosody Sensor (Jitter/Shimmer) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** Not implemented
**Effort:** Medium (2-3 days)
**Trial Priority:** HIGH

#### The Physics

Natural voice has micro-variations called **jitter** (frequency perturbation) and **shimmer** (amplitude perturbation):

- **Jitter**: Cycle-to-cycle F0 variation (healthy range: 0.5-1.0%)
- **Shimmer**: Cycle-to-cycle amplitude variation (healthy range: 3-7%)

Synthetic voices often have:
- Too little jitter/shimmer (TTS)
- Abnormal jitter/shimmer patterns (voice conversion)

#### Detection Strategy

```python
class MicroProsodySensor(BaseSensor):
    """
    Analyzes voice quality micro-perturbations (jitter/shimmer).

    The Physics:
    - Real voice has natural F0/amplitude micro-variations
    - TTS produces too-smooth pitch contours
    - Voice conversion introduces characteristic artifacts

    Patent Safety: Uses cycle-to-cycle analysis (kinematic), not LPC.
    """

    def analyze(self, audio_data: np.ndarray, samplerate: int) -> SensorResult:
        # 1. Extract voiced segments
        voiced_frames = self._extract_voiced(audio_data, samplerate)

        # 2. Calculate local jitter (cycle-to-cycle F0 variation)
        jitter = self._calculate_jitter(voiced_frames, samplerate)

        # 3. Calculate local shimmer (cycle-to-cycle amplitude variation)
        shimmer = self._calculate_shimmer(voiced_frames)

        # 4. Score based on expected ranges
        # Too low jitter/shimmer = synthetic
        # Abnormal patterns = artifacts
```

#### Patent Safety

‚úÖ **SAFE**:
- Cycle-to-cycle analysis (kinematic)
- No spectral modeling required
- Uses time-domain perturbation measures

---

### 2.3 Prosodic Velocity Sensor (F0 Dynamics) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** Not implemented
**Effort:** Low (1-2 days)
**Trial Priority:** HIGH

#### The Physics

Extends the existing **Pitch Velocity Sensor** to analyze F0 contour dynamics more comprehensively:

- **F0 acceleration**: Rate of pitch change velocity
- **F0 reset patterns**: How pitch resets between phrases
- **Declination patterns**: Natural pitch lowering over utterances

#### Detection Strategy

```python
class ProsodicVelocitySensor(BaseSensor):
    """
    Analyzes pitch contour dynamics for naturalness.

    Complements PitchVelocitySensor by examining:
    - F0 acceleration (second derivative)
    - Phrase-boundary pitch patterns
    - Natural declination trends

    Patent Safety: Uses velocity/acceleration (kinematics), not static values.
    """

    def analyze(self, audio_data: np.ndarray, samplerate: int) -> SensorResult:
        # 1. Extract F0 contour
        f0 = self._extract_f0(audio_data, samplerate)

        # 2. Calculate F0 velocity (already done by PitchVelocitySensor)
        f0_velocity = np.gradient(f0)

        # 3. Calculate F0 acceleration (new)
        f0_acceleration = np.gradient(f0_velocity)

        # 4. Check acceleration limits (human physiology)
        # Max laryngeal acceleration ~50 semitones/sec¬≤
        max_acceleration = np.max(np.abs(f0_acceleration))

        # 5. Analyze phrase patterns
        declination = self._analyze_declination(f0)
```

#### Patent Safety

‚úÖ **SAFE**: Uses kinematic analysis (derivatives), not static spectral values.

---

### 2.4 Temporal Envelope Sensor (Attack/Decay) ‚≠ê‚≠ê‚≠ê

**Status:** Not implemented
**Effort:** Low (1-2 days)
**Trial Priority:** MEDIUM

#### The Physics

Complements the **Glottal Inertia Sensor** by analyzing the complete temporal envelope:

- **Attack time**: How quickly sounds begin
- **Decay time**: How sounds fade
- **Attack-decay ratio**: Characteristic of different phonemes

#### Detection Strategy

```python
class TemporalEnvelopeSensor(BaseSensor):
    """
    Analyzes attack/decay patterns of audio segments.

    The Physics:
    - Biological articulators have inertia affecting attack/decay
    - Different phonemes have characteristic envelope shapes
    - Synthetic audio may have unnatural envelope patterns

    Complements GlottalInertiaSensor with broader envelope analysis.
    """
```

#### Patent Safety

‚úÖ **SAFE**: Temporal envelope analysis is kinematic/dynamic, not spectral.

---

## Part 3: Implementation Roadmap

### Phase 1: Quick Wins (Week 1)

| Task | Effort | Impact | Risk |
|------|--------|--------|------|
| Activate ENF Sensor | 1 hour | High | Low |
| Activate Breathing Pattern Sensor | 1 hour | High | Low |
| Adjust sensor weights | 2 hours | Medium | Low |

### Phase 2: Calibration (Week 2)

| Task | Effort | Impact | Risk |
|------|--------|--------|------|
| Two-Mouth threshold tuning | 4 hours | High | Medium |
| Coarticulation weight testing | 4 hours | Medium | Low |
| A/B testing framework | 8 hours | High | Low |

### Phase 3: New Sensors (Weeks 3-4)

| Task | Effort | Impact | Risk |
|------|--------|--------|------|
| Spectral Harmonicity Sensor | 16 hours | High | Medium |
| Micro-Prosody Sensor | 16 hours | High | Medium |
| Prosodic Velocity Sensor | 8 hours | Medium | Low |
| Temporal Envelope Sensor | 8 hours | Medium | Low |

---

## Part 4: Testing Strategy

### Calibration Dataset Requirements

Current dataset: ~50 samples (insufficient)

**Recommended expansion:**

| Category | Current | Target | Source |
|----------|---------|--------|--------|
| Real speech | ~25 | 200 | Common Voice, LibriSpeech |
| TTS synthetic | ~15 | 100 | Various TTS engines |
| Voice conversion | ~10 | 50 | Voice cloning services |
| Real with noise | 0 | 50 | Augmented real samples |

### A/B Testing Framework

```python
# Proposed testing approach:
class SensorABTest:
    def __init__(self, sensor_a: BaseSensor, sensor_b: BaseSensor):
        self.sensor_a = sensor_a
        self.sensor_b = sensor_b

    def run_comparison(self, test_samples: List[LabeledSample]):
        results_a = []
        results_b = []

        for sample in test_samples:
            result_a = self.sensor_a.analyze(sample.audio, sample.sr)
            result_b = self.sensor_b.analyze(sample.audio, sample.sr)

            results_a.append((result_a.passed, sample.is_real))
            results_b.append((result_b.passed, sample.is_real))

        return self.calculate_metrics(results_a, results_b)
```

---

## Part 5: Risk Assessment

### Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| New sensors decrease accuracy | Medium | High | A/B testing, gradual rollout |
| ENF not present in all recordings | High | Medium | Make optional, document limitations |
| Breathing pattern fails on short audio | High | Low | Minimum duration check (exists) |
| Two-Mouth still too aggressive | Medium | Medium | Conservative threshold adjustment |

### Patent Risks

| New Sensor | Patent Risk | Mitigation |
|------------|-------------|------------|
| Spectral Harmonicity | Low | Uses autocorrelation, not LPC |
| Micro-Prosody | Low | Kinematic analysis only |
| Prosodic Velocity | Low | Extends existing safe approach |
| Temporal Envelope | Low | Time-domain only |

All proposed sensors use **kinematic/dynamic analysis** (velocities, accelerations, variances) rather than **static spectral values**, maintaining the patent-safe "Motor-Planning & Phase-Physics Model."

---

## Appendix A: Configuration Changes

### Recommended settings.yaml Updates

```yaml
sensors:
  enf:
    enabled: true
    nominal_frequency: 60.0  # 50.0 for EU/Asia
    min_duration_sec: 2.0
    category: "defense"
    weight: 0.08

  breathing_pattern:
    enabled: true
    min_variance: 0.3
    snr_threshold_db: 10.0
    category: "defense"
    weight: 0.08

  two_mouth:
    enabled: true  # Re-enable after calibration
    vtl_variance_threshold: 0.15  # Relaxed from 0.10
    spectral_conflict_threshold: 0.08  # Relaxed from 0.05
    category: "prosecution"
    weight: 0.08  # Reduced from 0.15

  coarticulation:
    weight: 0.10  # Increased from 0.05

  # New sensors (Phase 3)
  spectral_harmonicity:
    enabled: false  # Enable after implementation
    hnr_max_threshold: 20.0
    hnr_variance_min: 5.0
    category: "prosecution"
    weight: 0.12
```

---

## Appendix B: Quick Activation Script

```python
# Script to activate underutilized sensors for trial
# Run: python scripts/activate_trial_sensors.py

def update_registry():
    """Add ENF and Breathing Pattern sensors to default pipeline."""

    # Backup current registry.py
    # Edit get_default_sensors() to include:

    new_imports = """
from .enf import ENFSensor
from .breathing_pattern import BreathingPatternSensor
"""

    new_sensors = """
        ENFSensor(category="defense"),              # Trust (Environmental)
        BreathingPatternSensor(category="defense"), # Trust (Natural Timing)
"""

    print("Add to backend/sensors/registry.py:")
    print(new_imports)
    print("Add to sensor list:")
    print(new_sensors)

if __name__ == "__main__":
    update_registry()
```

---

## Part 6: Complete Sensor Activation Matrix

### Current State (21 Total Sensors Implemented)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Sensor                              ‚îÇ Impl  ‚îÇ Default  ‚îÇ Weight ‚îÇ Notes                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ GlottalInertiaSensor                ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.35   ‚îÇ Primary prosecution sensor      ‚îÇ
‚îÇ PitchVelocitySensor                 ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.15   ‚îÇ Laryngeal mechanics             ‚îÇ
‚îÇ FormantTrajectorySensor             ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.20   ‚îÇ Spectral velocity               ‚îÇ
‚îÇ DigitalSilenceSensor                ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.15   ‚îÇ Splicing detection              ‚îÇ
‚îÇ GlobalFormantSensor                 ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.10   ‚îÇ Spectral envelope               ‚îÇ
‚îÇ CoarticulationSensor                ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.05   ‚îÇ Motor planning                  ‚îÇ
‚îÇ BreathSensor                        ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.30*  ‚îÇ *Fallback weight only           ‚îÇ
‚îÇ DynamicRangeSensor                  ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.00*  ‚îÇ *Info-only sensor               ‚îÇ
‚îÇ BandwidthSensor                     ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.00*  ‚îÇ *Context detection trigger      ‚îÇ
‚îÇ TwoMouthSensor                      ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.00** ‚îÇ **DISABLED: 50% false positive  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ENFSensor                           ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ NOT REGISTERED in pipeline      ‚îÇ
‚îÇ BreathingPatternSensor              ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ NOT REGISTERED in pipeline      ‚îÇ
‚îÇ PhaseCoherenceSensor                ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ Replaced by PitchVelocity       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ HFEnsembleSensor                    ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ 0.00   ‚îÇ DISABLED: Stability issues      ‚îÇ
‚îÇ HuggingFaceDetectorSensor           ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ Requires HF_TOKEN               ‚îÇ
‚îÇ RawNet3Stage (Neural)               ‚îÇ ‚úÖ    ‚îÇ ‚úÖ       ‚îÇ 0.40   ‚îÇ Deep learning (full pipeline)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Rust: VacuumSensor                  ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ Optional performance            ‚îÇ
‚îÇ Rust: PhaseSensor                   ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ Optional performance            ‚îÇ
‚îÇ Rust: ArticulationSensor            ‚îÇ ‚úÖ    ‚îÇ ‚ùå       ‚îÇ ‚Äî      ‚îÇ Optional performance            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Gap Analysis: Documentation vs Implementation

| Claim (Strategic Analysis) | Reality | Status |
|---------------------------|---------|--------|
| "78.6% overall accuracy" | 100% on legitimate, **0% on deepfakes** | ‚ö†Ô∏è MISLEADING |
| "10+ physics-based sensors" | 10 in pipeline, 3 dormant | ‚úÖ Accurate |
| "ENF sensor integrated" | Implemented but NOT in default pipeline | ‚ùå FALSE |
| "Breathing pattern analysis" | Implemented but NOT activated | ‚ùå FALSE |
| "Dual-factor verification" | Active (prosecution/defense) | ‚úÖ Accurate |
| "Sub-second latency" | 0.026s for 0.5s audio | ‚úÖ Verified |
| "Production-ready" | `demo_mode: true` in settings.yaml | ‚ö†Ô∏è NOT PRODUCTION |

---

## Part 7: Revised Priority Actions

### IMMEDIATE (This Week)

| Priority | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P0** | Fix decision threshold (ESCALATE at 0.35, not 0.5) | **Fixes 0% ‚Üí ~80% detection** | 1 hour |
| **P0** | Remove 0.5 dilution factor in composite_risk | **Direct voice_risk usage** | 30 mins |
| **P1** | Activate ENFSensor in get_default_sensors() | +10% accuracy on recordings | 1 hour |
| **P1** | Activate BreathingPatternSensor | +5% accuracy on TTS | 1 hour |
| **P1** | Set demo_mode: false | Production configuration | 5 mins |

### SHORT-TERM (2 Weeks)

| Priority | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P2** | Recalibrate TwoMouthSensor thresholds | +8% detection, -40% false positives | 4 hours |
| **P2** | Expand calibration dataset (50 ‚Üí 200 samples) | Reliable metrics | 8 hours |
| **P2** | Add PhaseCoherenceSensor as backup | Redundancy for vocoders | 2 hours |
| **P3** | Enable HFEnsembleSensor (low weight 0.05) | ML backup | 2 hours |

### MEDIUM-TERM (4 Weeks)

| Priority | Action | Impact | Effort |
|----------|--------|--------|--------|
| **P3** | Implement SpectralHarmonicitySensor | +10% on neural TTS | 16 hours |
| **P3** | Implement MicroProsodySensor | +8% on clones | 16 hours |
| **P4** | Benchmark against ASVspoof 2021 | Industry validation | 8 hours |

---

## Conclusion

The Sonotheia codebase reveals a **sophisticated detection architecture with critical operational gaps**:

### What's Working Well
- ‚úÖ 21 detection methods implemented (physics, ML, neural)
- ‚úÖ Patent-safe architecture (no LPC, kinematic analysis)
- ‚úÖ Sensors correctly identify suspicious audio (voice_risk 0.76-0.99)
- ‚úÖ Dual-factor prosecution/defense model is sound
- ‚úÖ Sub-second latency achieved

### Critical Issues Requiring Immediate Action
1. **üö® Decision threshold bug**: Deepfakes approved due to score dilution (0% detection rate)
2. **3 dormant sensors**: ENF, BreathingPattern, PhaseCoherence implemented but not activated
3. **4 disabled sensors**: TwoMouth, HFDeepfake, HFEnsemble disabled (weight=0)
4. **Minimal calibration**: Only 14 samples (3 deepfakes) - statistically meaningless
5. **Demo mode active**: `demo_mode: true` in production config

### Recommended Approach

**Day 1 (CRITICAL):**
- Fix decision threshold: `ESCALATE >= 0.35` instead of `>= 0.5`
- Remove composite_risk dilution factor
- Set `demo_mode: false`

**Week 1:**
- Activate ENFSensor and BreathingPatternSensor
- Run expanded calibration with 50+ deepfake samples

**Week 2:**
- Recalibrate TwoMouthSensor thresholds
- Enable PhaseCoherenceSensor as backup

**Weeks 3-4:**
- Implement SpectralHarmonicitySensor
- Benchmark against ASVspoof 2021 dataset

All recommendations maintain **patent safety** by using kinematic/dynamic analysis rather than static spectral or LPC-based methods.

---

## Appendix C: Website vs Reality

**sonotheia.ai claims**: "Defensible and explainable voice fraud mitigation solutions for regulated industries"

| Website Claim | Codebase Reality |
|--------------|------------------|
| "Defensible" | ‚ö†Ô∏è 0% deepfake detection in calibration |
| "Explainable" | ‚úÖ Physics-based sensors with clear violation reasons |
| "Regulated industries" | ‚úÖ SAR generation, compliance architecture present |
| "Voice fraud mitigation" | ‚ö†Ô∏è Currently approving all fraudulent audio |

**Recommendation:** Do not deploy to production until decision threshold bug is fixed and detection rate is validated at >70%.

---

**Document Maintainer:** Sonotheia Development Team
**Version:** 2.0 (Revised with critical findings)
**Next Review:** After P0 fixes implemented
